{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e436e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 14:53:03.297881: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-17 14:53:03.410603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-17 14:53:03.410646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-17 14:53:03.412202: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-17 14:53:03.424833: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-17 14:53:03.426208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-17 14:53:05.633598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from scvi.module.base import PyroBaseModuleClass\n",
    "from scvi.train import PyroTrainingPlan\n",
    "from typing import Optional, Union\n",
    "import pyro\n",
    "\n",
    "max_epochs = 4000\n",
    "start_lr = 0.01\n",
    "final_lr = 0.001\n",
    "lrd = (final_lr/start_lr)**(1/max_epochs)\n",
    "clipped_adam = pyro.optim.ClippedAdam({\"lr\": start_lr, \"lrd\": lrd,  \"clip_norm\": 10.0})\n",
    "\n",
    "class PyroTrainingPlan_ClippedAdamDecayingRate(PyroTrainingPlan):\n",
    "    \"\"\"\n",
    "    Lightning module task to train Pyro scvi-tools modules.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pyro_module\n",
    "        An instance of :class:`~scvi.module.base.PyroBaseModuleClass`. This object\n",
    "        should have callable `model` and `guide` attributes or methods.\n",
    "    loss_fn\n",
    "        A Pyro loss. Should be a subclass of :class:`~pyro.infer.ELBO`.\n",
    "        If `None`, defaults to :class:`~pyro.infer.Trace_ELBO`.\n",
    "    optim\n",
    "        A Pyro optimizer instance, e.g., :class:`~pyro.optim.Adam`. If `None`,\n",
    "        defaults to :class:`pyro.optim.Adam` optimizer with a learning rate of `1e-3`.\n",
    "    optim_kwargs\n",
    "        Keyword arguments for **default** optimiser :class:`pyro.optim.Adam`.\n",
    "    n_aggressive_epochs\n",
    "        Number of epochs in aggressive optimisation of amortised variables.\n",
    "    n_aggressive_steps\n",
    "        Number of steps to spend optimising amortised variables before one step optimising global variables.\n",
    "    n_steps_kl_warmup\n",
    "        Number of training steps (minibatches) to scale weight on KL divergences from 0 to 1.\n",
    "        Only activated when `n_epochs_kl_warmup` is set to None.\n",
    "    n_epochs_kl_warmup\n",
    "        Number of epochs to scale weight on KL divergences from 0 to 1.\n",
    "        Overrides `n_steps_kl_warmup` when both are not `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pyro_module: PyroBaseModuleClass,\n",
    "        loss_fn: Optional[pyro.infer.ELBO] = None,\n",
    "        optim: Optional[pyro.optim.PyroOptim] = clipped_adam,\n",
    "        optim_kwargs: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            pyro_module=pyro_module,\n",
    "            loss_fn=loss_fn,\n",
    "            optim=optim,\n",
    "            optim_kwargs=optim_kwargs\n",
    "        )\n",
    "\n",
    "        self.svi = pyro.infer.SVI(\n",
    "            model=pyro_module.model,\n",
    "            guide=pyro_module.guide,\n",
    "            optim=self.optim,\n",
    "            loss=self.loss_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e59a357",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scvi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLowLevelPyroTrainingPlan\u001b[39;00m(\u001b[43mscvi\u001b[49m\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mLowLevelPyroTrainingPlan):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      4\u001b[0m         lr_scheduler_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m      6\u001b[0m     ):\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scvi' is not defined"
     ]
    }
   ],
   "source": [
    "import scvi\n",
    "\n",
    "class LowLevelPyroTrainingPlan(scvi.train.LowLevelPyroTrainingPlan):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lr_scheduler_kwargs=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lr_scheduler_kwargs = lr_scheduler_kwargs\n",
    "\n",
    "    def configure_optimizers(self): \n",
    "        parameters = self.module.named_parameters()\n",
    "        optimizer = self.optim(parameters, **self.optim_kwargs)\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer=optimizer,\n",
    "            **self.lr_scheduler_kwargs,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "def _get_cosine_schedule_with_warmup_lr_lambda(\n",
    "    current_step: int,\n",
    "    *,\n",
    "    num_warmup_steps: int,\n",
    "    num_training_steps: int,\n",
    "    num_cycles: float,\n",
    "):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    progress = float(current_step - num_warmup_steps) / float(\n",
    "        max(1, num_training_steps - num_warmup_steps)\n",
    "    )\n",
    "    return max(\n",
    "        0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n",
    "    )\n",
    "\n",
    "\n",
    "def get_cosine_schedule_with_warmup(\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_warmup_steps: int,\n",
    "    num_training_steps: int,\n",
    "    num_cycles: float = 0.5,\n",
    "    last_epoch: int = -1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases following the values of the cosine function between the\n",
    "    initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n",
    "    initial lr set in the optimizer.\n",
    "\n",
    "    https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.get_cosine_schedule_with_warmup\n",
    "\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "        num_cycles (`float`, *optional*, defaults to 0.5):\n",
    "            The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n",
    "            following a half-cosine).\n",
    "        last_epoch (`int`, *optional*, defaults to -1):\n",
    "            The index of the last epoch when resuming training.\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    lr_lambda = partial(\n",
    "        _get_cosine_schedule_with_warmup_lr_lambda,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        num_cycles=num_cycles,\n",
    "    )\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5023c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "scvi.settings.seed = 1\n",
    "import cell2fate as c2f\n",
    "import pickle as pickle\n",
    "from eval_utils import cross_boundary_correctness\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import unitvelo as utv\n",
    "method = 'Cell2fateDynamicalModel_VitaliiTrainingPlan'\n",
    "data_dir = '/nfs/team283/aa16/data/fate_benchmarking/benchmarking_datasets/'\n",
    "save_dir = '/nfs/team283/aa16/data/fate_benchmarking/benchmarking_results/'\n",
    "datasets = ['Pancreas_with_cc',  'DentateGyrus' , 'MouseBoneMarrow', 'MouseErythroid', 'HumanBoneMarrow']\n",
    "n_genes_list = np.array((2000, 3000))\n",
    "n_counts_list = np.array((10, 20))\n",
    "\n",
    "for i in range(len(n_genes_list)):\n",
    "    for j in range(len(n_counts_list)):\n",
    "        for k in (2,0,1,3,4):\n",
    "            print(i)\n",
    "            print(j)\n",
    "            print(k)\n",
    "            dataset = datasets[k]\n",
    "            n_genes = n_genes_list[i]\n",
    "            min_counts = n_counts_list[j]\n",
    "            model_index = str(i) + '-' + str(j) + '-' + str(k)\n",
    "            save_name = method + '_'\n",
    "            if exists(save_dir + save_name + '_CBDC_fullBenchmark.csv'):\n",
    "                tab = pd.read_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv', index_col = 0)\n",
    "                if model_index in tab.index:\n",
    "                    continue\n",
    "            adata = sc.read_h5ad(data_dir + dataset + '/' + dataset + '_anndata.h5ad')\n",
    "            adata = c2f.utils.get_training_data(adata, cells_per_cluster = 10**6, cluster_column = 'clusters',\n",
    "                                            remove_clusters = [], min_shared_counts = min_counts, n_var_genes= n_genes)\n",
    "            c2f.Cell2fate_DynamicalModel.setup_anndata(adata, spliced_label='spliced', unspliced_label='unspliced')    \n",
    "            n_modules = c2f.utils.get_max_modules(adata)\n",
    "            mod = c2f.Cell2fate_DynamicalModel(adata,\n",
    "                                               n_modules = n_modules)\n",
    "            mod.train(max_epochs = 4000, **{'training_plan' : PyroTrainingPlan_ClippedAdamDecayingRate},\n",
    "                     early_stopping = True, early_stopping_min_delta = 10**(-4),\n",
    "                     early_stopping_monitor = 'elbo_train', early_stopping_patience = 45)\n",
    "            sample_kwarg = {\"num_samples\": 10, \"batch_size\" : adata.n_obs,\n",
    "                 \"use_gpu\" : True, 'return_samples': False}\n",
    "            adata = mod.export_posterior(adata, sample_kwargs = sample_kwarg)\n",
    "            mod.compute_and_plot_total_velocity_scvelo(adata, save = False, delete = False)\n",
    "            # Calculate performance metrics:\n",
    "            file = open(data_dir + dataset + '/' + dataset + '_groundTruth.pickle' ,'rb')\n",
    "            ground_truth = pickle.load(file)\n",
    "            metrics = utv.evaluate(adata, ground_truth, 'clusters', 'velocity')\n",
    "            cb_score = [np.mean(metrics['Cross-Boundary Direction Correctness (A->B)'][x])\n",
    "                        for x in metrics['Cross-Boundary Direction Correctness (A->B)'].keys()]\n",
    "            if exists(save_dir + save_name + '_CBDC_fullBenchmark.csv'):\n",
    "                tab = pd.read_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv', index_col = 0)\n",
    "            else:\n",
    "                c_names = ['CBDC']\n",
    "                tab = pd.DataFrame(columns = c_names)\n",
    "            tab.loc[model_index, 'CBDC'] = np.mean(cb_score)\n",
    "            tab.to_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv')\n",
    "tab = pd.read_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv', index_col = 0)\n",
    "tab.loc['AVERAGE', 'CBDC'] = np.mean(tab['CBDC'])\n",
    "tab.to_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42097d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (cell2fate_env)",
   "language": "python",
   "name": "cell2fate_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
