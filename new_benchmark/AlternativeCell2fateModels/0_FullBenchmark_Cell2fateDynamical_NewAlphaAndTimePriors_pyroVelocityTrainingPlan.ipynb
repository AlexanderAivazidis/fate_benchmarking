{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a6e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from scvi.module.base import PyroBaseModuleClass\n",
    "from scvi.train import PyroTrainingPlan\n",
    "from typing import Optional, Union\n",
    "import pyro\n",
    "\n",
    "max_epochs = 4000\n",
    "start_lr = 0.01\n",
    "final_lr = 0.001\n",
    "lrd = (final_lr/start_lr)**(1/max_epochs)\n",
    "clipped_adam = pyro.optim.ClippedAdam({\"lr\": start_lr, \"lrd\": lrd,  \"clip_norm\": 10.0})\n",
    "\n",
    "class PyroTrainingPlan_ClippedAdamDecayingRate(PyroTrainingPlan):\n",
    "    \"\"\"\n",
    "    Lightning module task to train Pyro scvi-tools modules.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pyro_module\n",
    "        An instance of :class:`~scvi.module.base.PyroBaseModuleClass`. This object\n",
    "        should have callable `model` and `guide` attributes or methods.\n",
    "    loss_fn\n",
    "        A Pyro loss. Should be a subclass of :class:`~pyro.infer.ELBO`.\n",
    "        If `None`, defaults to :class:`~pyro.infer.Trace_ELBO`.\n",
    "    optim\n",
    "        A Pyro optimizer instance, e.g., :class:`~pyro.optim.Adam`. If `None`,\n",
    "        defaults to :class:`pyro.optim.Adam` optimizer with a learning rate of `1e-3`.\n",
    "    optim_kwargs\n",
    "        Keyword arguments for **default** optimiser :class:`pyro.optim.Adam`.\n",
    "    n_aggressive_epochs\n",
    "        Number of epochs in aggressive optimisation of amortised variables.\n",
    "    n_aggressive_steps\n",
    "        Number of steps to spend optimising amortised variables before one step optimising global variables.\n",
    "    n_steps_kl_warmup\n",
    "        Number of training steps (minibatches) to scale weight on KL divergences from 0 to 1.\n",
    "        Only activated when `n_epochs_kl_warmup` is set to None.\n",
    "    n_epochs_kl_warmup\n",
    "        Number of epochs to scale weight on KL divergences from 0 to 1.\n",
    "        Overrides `n_steps_kl_warmup` when both are not `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pyro_module: PyroBaseModuleClass,\n",
    "        loss_fn: Optional[pyro.infer.ELBO] = None,\n",
    "        optim: Optional[pyro.optim.PyroOptim] = clipped_adam,\n",
    "        optim_kwargs: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            pyro_module=pyro_module,\n",
    "            loss_fn=loss_fn,\n",
    "            optim=optim,\n",
    "            optim_kwargs=optim_kwargs\n",
    "        )\n",
    "\n",
    "        self.svi = pyro.infer.SVI(\n",
    "            model=pyro_module.model,\n",
    "            guide=pyro_module.guide,\n",
    "            optim=self.optim,\n",
    "            loss=self.loss_fn,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5023c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Running UniTVelo 0.2.5.2)\n",
      "2023-11-16 16:42:35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 16:42:37.599021: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-16 16:42:37.599090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-16 16:42:37.759706: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "2\n",
      "Keeping at most 1000000 cells per cluster\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 19601 genes that are detected 10 counts (shared).\n",
      "Skip filtering by dispersion since number of variables are less than `n_top_genes`.\n",
      "Leiden clustering ...\n",
      "Number of Leiden Clusters: 8\n",
      "Maximal Number of Modules: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-GPU-c92e82f0-acdb-bfba-5c7c-90306242f281/5/0]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m\n\u001b[1;32m     44\u001b[0m n_modules \u001b[38;5;241m=\u001b[39m c2f\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_max_modules(adata)\n\u001b[1;32m     45\u001b[0m mod \u001b[38;5;241m=\u001b[39m c2f\u001b[38;5;241m.\u001b[39mCell2fate_DynamicalModel(adata,\n\u001b[1;32m     46\u001b[0m                                    n_modules \u001b[38;5;241m=\u001b[39m n_modules,\n\u001b[1;32m     47\u001b[0m                                    Tmax_prior\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m300.\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msd\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100.\u001b[39m},\n\u001b[1;32m     48\u001b[0m                                    factor_prior\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2.\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstates_per_gene\u001b[39m\u001b[38;5;124m\"\u001b[39m: n_modules\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.8\u001b[39m})\n\u001b[0;32m---> 49\u001b[0m \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining_plan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPyroTrainingPlan_ClippedAdamDecayingRate\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m         \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_min_delta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m         \u001b[49m\u001b[43mearly_stopping_monitor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43melbo_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m sample_kwarg \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m : adata\u001b[38;5;241m.\u001b[39mn_obs,\n\u001b[1;32m     53\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_gpu\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_samples\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[1;32m     54\u001b[0m adata \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mexport_posterior(adata, sample_kwargs \u001b[38;5;241m=\u001b[39m sample_kwarg)\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/cell2fate/_cell2fate_DynamicalModel.py:162\u001b[0m, in \u001b[0;36mCell2fate_DynamicalModel.train\u001b[0;34m(self, max_epochs, batch_size, train_size, lr, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m train_size\n\u001b[1;32m    160\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m lr\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/scvi/model/base/_pyromixin.py:146\u001b[0m, in \u001b[0;36mPyroSviTrainMixin.train\u001b[0;34m(self, max_epochs, use_gpu, train_size, validation_size, batch_size, early_stopping, lr, training_plan, plan_kwargs, **trainer_kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m trainer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(PyroJitGuideWarmup())\n\u001b[1;32m    138\u001b[0m runner \u001b[38;5;241m=\u001b[39m TrainRunner(\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    140\u001b[0m     training_plan\u001b[38;5;241m=\u001b[39mtraining_plan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_kwargs,\n\u001b[1;32m    145\u001b[0m )\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/scvi/train/_trainrunner.py:74\u001b[0m, in \u001b[0;36mTrainRunner.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_splitter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_val\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_plan\u001b[38;5;241m.\u001b[39mn_obs_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_splitter\u001b[38;5;241m.\u001b[39mn_val\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_plan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_splitter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# data splitter only gets these attrs after fit\u001b[39;00m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/scvi/train/_trainer.py:186\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], PyroTrainingPlan):\n\u001b[1;32m    181\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m    182\u001b[0m         action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    183\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    184\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    185\u001b[0m     )\n\u001b[0;32m--> 186\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:740\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    735\u001b[0m     rank_zero_deprecation(\n\u001b[1;32m    736\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`trainer.fit(train_dataloader)` is deprecated in v1.4 and will be removed in v1.6.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Use `trainer.fit(train_dataloaders)` instead. HINT: added \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m     )\n\u001b[1;32m    739\u001b[0m     train_dataloaders \u001b[38;5;241m=\u001b[39m train_dataloader\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:685\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;124;03mError handling, intended to be used only for main trainer function entry points (fit, validate, test, predict)\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;124;03mas all errors should funnel through them\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    **kwargs: keyword arguments to be passed to `trainer_fn`\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:777\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# TODO: ckpt_path only in v1.7\u001b[39;00m\n\u001b[1;32m    776\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1199\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;66;03m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;66;03m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1279\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_type_plugin\u001b[38;5;241m.\u001b[39mstart_predicting(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_training\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py:202\u001b[0m, in \u001b[0;36mTrainingTypePlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.Trainer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# double dispatch to initiate the training loop\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1289\u001b[0m, in \u001b[0;36mTrainer.run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1319\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/loops/base.py:140\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_skip()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_start\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:197\u001b[0m, in \u001b[0;36mFitLoop.on_run_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the ``on_train_start`` hook.\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# reset train dataloader and val dataloader\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_train_val_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_fresh_start_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_results\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:595\u001b[0m, in \u001b[0;36mTrainerDataLoadingMixin.reset_train_val_dataloaders\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets train and val dataloaders if none are attached to the trainer.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03mThe val dataloader must be initialized before training loop starts, as the training loop\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    model: The `LightningModule` if called outside of the trainer scope.\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_dataloaders \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_val_dataloader(model\u001b[38;5;241m=\u001b[39mmodel)\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:391\u001b[0m, in \u001b[0;36mTrainerDataLoadingMixin.reset_train_dataloader\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader \u001b[38;5;241m=\u001b[39m CombinedLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mmultiple_trainloader_mode)\n\u001b[1;32m    388\u001b[0m module \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatamodule\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_training_batches \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader)\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mhas_len_all_ranks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_type_plugin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit_train_batches, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit_train_batches \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_training_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_training_batches, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimit_train_batches))\n",
      "File \u001b[0;32m/nfs/team283/aa16/software/miniconda3/envs/cell2fate_env/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:117\u001b[0m, in \u001b[0;36mhas_len_all_ranks\u001b[0;34m(dataloader, training_type, model)\u001b[0m\n\u001b[1;32m    114\u001b[0m total_length \u001b[38;5;241m=\u001b[39m training_type\u001b[38;5;241m.\u001b[39mreduce(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlen\u001b[39m(dataloader))\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice), reduce_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m local_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtotal_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal length of `Dataloader` across ranks is zero. Please make sure that it returns at least 1 batch.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m     )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m local_length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.chdir('..')\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import scvi\n",
    "scvi.settings.seed = 1\n",
    "import cell2fate as c2f\n",
    "import pickle as pickle\n",
    "from eval_utils import cross_boundary_correctness\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import unitvelo as utv\n",
    "method = 'Cell2fateDynamicalModel__NewAlphaAndTimePriors_pyroVelocityTrainingPlan'\n",
    "data_dir = '/nfs/team283/aa16/data/fate_benchmarking/benchmarking_datasets/'\n",
    "save_dir = '/nfs/team283/aa16/data/fate_benchmarking/benchmarking_results/'\n",
    "datasets = ['Pancreas_with_cc',  'DentateGyrus' , 'MouseBoneMarrow', 'MouseErythroid', 'HumanBoneMarrow']\n",
    "n_genes_list = np.array((2000, 3000))\n",
    "n_counts_list = np.array((10, 20))\n",
    "\n",
    "for i in range(len(n_genes_list)):\n",
    "    for j in range(len(n_counts_list)):\n",
    "        for k in (2,0,1,3,4):\n",
    "            print(i)\n",
    "            print(j)\n",
    "            print(k)\n",
    "            dataset = datasets[k]\n",
    "            n_genes = n_genes_list[i]\n",
    "            min_counts = n_counts_list[j]\n",
    "            model_index = str(i) + '-' + str(j) + '-' + str(k)\n",
    "            save_name = method + '_'\n",
    "            if exists(save_dir + save_name + '_CBDC_fullBenchmark.csv'):\n",
    "                tab = pd.read_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv', index_col = 0)\n",
    "                if model_index in tab.index:\n",
    "                    continue\n",
    "            adata = sc.read_h5ad(data_dir + dataset + '/' + dataset + '_anndata.h5ad')\n",
    "            adata = c2f.utils.get_training_data(adata, cells_per_cluster = 10**6, cluster_column = 'clusters',\n",
    "                                            remove_clusters = [], min_shared_counts = min_counts, n_var_genes= n_genes)\n",
    "            c2f.Cell2fate_DynamicalModel.setup_anndata(adata, spliced_label='spliced', unspliced_label='unspliced')    \n",
    "            n_modules = c2f.utils.get_max_modules(adata)\n",
    "            mod = c2f.Cell2fate_DynamicalModel(adata,\n",
    "                                               n_modules = n_modules,\n",
    "                                               Tmax_prior={\"mean\": 300., \"sd\": 100.},\n",
    "                                               factor_prior={\"rate\": 2., \"alpha\": 0.2, \"states_per_gene\": n_modules*0.8})\n",
    "            mod.train(max_epochs = 4000, **{'training_plan' : PyroTrainingPlan_ClippedAdamDecayingRate},\n",
    "                     early_stopping = True, early_stopping_min_delta = 10**(-4),\n",
    "                     early_stopping_monitor = 'elbo_train', early_stopping_patience = 45)\n",
    "            sample_kwarg = {\"num_samples\": 10, \"batch_size\" : adata.n_obs,\n",
    "                 \"use_gpu\" : True, 'return_samples': False}\n",
    "            adata = mod.export_posterior(adata, sample_kwargs = sample_kwarg)\n",
    "            mod.compute_and_plot_total_velocity_scvelo(adata, save = False, delete = False)\n",
    "            # Calculate performance metrics:\n",
    "            file = open(data_dir + dataset + '/' + dataset + '_groundTruth.pickle' ,'rb')\n",
    "            ground_truth = pickle.load(file)\n",
    "            metrics = utv.evaluate(adata, ground_truth, 'clusters', 'velocity')\n",
    "            cb_score = [np.mean(metrics['Cross-Boundary Direction Correctness (A->B)'][x])\n",
    "                        for x in metrics['Cross-Boundary Direction Correctness (A->B)'].keys()]\n",
    "            if exists(save_dir + save_name + '_CBDC_fullBenchmark.csv'):\n",
    "                tab = pd.read_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv', index_col = 0)\n",
    "            else:\n",
    "                c_names = ['CBDC']\n",
    "                tab = pd.DataFrame(columns = c_names)\n",
    "            tab.loc[model_index, 'CBDC'] = np.mean(cb_score)\n",
    "            tab.to_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv')\n",
    "tab = pd.read_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv', index_col = 0)\n",
    "tab.loc['AVERAGE', 'CBDC'] = np.mean(tab['CBDC'])\n",
    "tab.to_csv(save_dir + save_name + '_CBDC_fullBenchmark.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42097d03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (cell2fate_env)",
   "language": "python",
   "name": "cell2fate_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
